@misc{abbottEstimatingTimevaryingReproduction2020a,
  title = {Estimating the Time-Varying Reproduction Number of {{SARS-CoV-2}} Using National and Subnational Case Counts},
  author = {Abbott, Sam and Hellewell, Joel and Thompson, Robin N. and Sherratt, Katharine and Gibbs, Hamish P. and Bosse, Nikos I. and Munday, James D. and Meakin, Sophie and Doughty, Emma L. and Chun, June Young and Chan, Yung-Wai Desmond and Finger, Flavio and Campbell, Paul and Endo, Akira and Pearson, Carl A. B. and Gimma, Amy and Russell, Tim and modelling Group, CMMID COVID and Flasche, Stefan and Kucharski, Adam J. and Eggo, Rosalind M. and Funk, Sebastian},
  year = {2020},
  month = jun,
  number = {5:112},
  institution = {{Wellcome Open Research}},
  doi = {10.12688/wellcomeopenres.16006.1},
  abstract = {Background: Interventions are now in place worldwide to reduce transmission of the novel coronavirus. Assessing temporal variations in transmission in different countries is essential for evaluating the effectiveness of public health interventions and the impact of changes in policy. Methods: We use case notification data to generate daily estimates of the time-dependent reproduction number in different regions and countries. Our modelling framework, based on open source tooling, accounts for reporting delays, so that temporal variations in reproduction number estimates can be compared directly with the times at which interventions are implemented. Results: We provide three example uses of our framework. First, we demonstrate how the toolset displays temporal changes in the reproduction number. Second, we show how the framework can be used to reconstruct case counts by date of infection from case counts by date of notification, as well as to estimate the reproduction number. Third, we show how maps can be generated to clearly show if case numbers are likely to decrease or increase in different regions. Results are shown for regions and countries worldwide on our website ( https://epiforecasts.io/covid/ ) and are updated daily. Our tooling is provided as an open-source R package to allow replication by others. Conclusions: This decision-support tool can be used to assess changes in virus transmission in different regions and countries worldwide. This allows policymakers to assess the effectiveness of current interventions, and will be useful for inferring whether or not transmission will increase when interventions are lifted. As well as providing daily updates on our website, we also provide adaptable computing code so that our approach can be used directly by researchers and policymakers on confidential datasets. We hope that our tool will be used to support decisions in countries worldwide throughout the ongoing COVID-19 pandemic.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {covid-19,forecasting,SARS-CoV-2,surveillance,time-varying reproduction number},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JTKE62HJ/Abbott et al. - 2020 - Estimating the time-varying reproduction number of.pdf}
}

@article{bosseComparingHumanModelbased2022,
  title = {Comparing Human and Model-Based Forecasts of {{COVID-19}} in {{Germany}} and {{Poland}}},
  author = {Bosse, Nikos I. and Abbott, Sam and Bracher, Johannes and Hain, Habakuk and Quilty, Billy J. and Jit, Mark and Group, Centre for the Mathematical Modelling of Infectious Diseases COVID-19 Working and van Leeuwen, Edwin and Cori, Anne and Funk, Sebastian},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010405},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010405},
  abstract = {Forecasts based on epidemiological modelling have played an important role in shaping public policy throughout the COVID-19 pandemic. This modelling combines knowledge about infectious disease dynamics with the subjective opinion of the researcher who develops and refines the model and often also adjusts model outputs. Developing a forecast model is difficult, resource- and time-consuming. It is therefore worth asking what modelling is able to add beyond the subjective opinion of the researcher alone. To investigate this, we analysed different real-time forecasts of cases of and deaths from COVID-19 in Germany and Poland over a 1-4 week horizon submitted to the German and Polish Forecast Hub. We compared crowd forecasts elicited from researchers and volunteers, against a) forecasts from two semi-mechanistic models based on common epidemiological assumptions and b) the ensemble of all other models submitted to the Forecast Hub. We found crowd forecasts, despite being overconfident, to outperform all other methods across all forecast horizons when forecasting cases (weighted interval score relative to the Hub ensemble 2 weeks ahead: 0.89). Forecasts based on computational models performed comparably better when predicting deaths (rel. WIS 1.26), suggesting that epidemiological modelling and human judgement can complement each other in important ways.},
  langid = {english},
  keywords = {Convolution,COVID 19,Data visualization,Epidemiology,Forecasting,Germany,Poland,Virus testing},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QZ74W5WX/Bosse et al. - 2022 - Comparing human and model-based forecasts of COVID.pdf}
}

@misc{bosseEvaluatingForecastsScoringutils2022,
  title = {Evaluating {{Forecasts}} with Scoringutils in {{R}}},
  author = {Bosse, Nikos I. and Gruson, Hugo and Cori, Anne and {van Leeuwen}, Edwin and Funk, Sebastian and Abbott, Sam},
  year = {2022},
  month = may,
  number = {arXiv:2205.07090},
  eprint = {2205.07090},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.07090},
  abstract = {Evaluating forecasts is essential in order to understand and improve forecasting and make forecasts useful to decision-makers. Much theoretical work has been done on the development of proper scoring rules and other scoring metrics that can help evaluate forecasts. In practice, however, conducting a forecast evaluation and comparison of different forecasters remains challenging. In this paper we introduce scoringutils, an R package that aims to greatly facilitate this process. It is especially geared towards comparing multiple forecasters, regardless of how forecasts were created, and visualising results. The package is able to handle missing forecasts and is the first R package to offer extensive support for forecasts represented through predictive quantiles, a format used by several collaborative ensemble forecasting efforts. The paper gives a short introduction to forecast evaluation, discusses the metrics implemented in scoringutils and gives guidance on when they are appropriate to use, and illustrates the application of the package using example data of forecasts for COVID-19 cases and deaths submitted to the European Forecast Hub between May and September 2021},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  annotation = {Preprint, \textbackslash url\{https://arxiv.org/abs/2205.07090\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QW8YLYQZ/Bosse et al. - 2022 - Evaluating Forecasts with scoringutils in R.pdf;/Users/nikos/github-synced/zotero-nikos/storage/DNAA9E73/2205.html}
}

@techreport{bosseTransformationForecastsEvaluating2023,
  type = {Preprint},
  title = {Transformation of Forecasts for Evaluating Predictive Performance in an Epidemiological Context},
  author = {Bosse, Nikos I. and Abbott, Sam and Cori, Anne and {van Leeuwen}, Edwin and Bracher, Johannes and Funk, Sebastian},
  year = {2023},
  month = jan,
  institution = {{Epidemiology}},
  doi = {10.1101/2023.01.23.23284722},
  abstract = {Abstract           Forecast evaluation plays an essential role in the development cycle of predictive epidemic models and can inform their use for public health decision-making. Common scores to evaluate epidemiological forecasts are the Continuous Ranked Probability Score (CRPS) and the Weighted Interval Score (WIS), which are both measures of the absolute distance between the forecast distribution and the observation. They are commonly applied directly to predicted and observed incidence counts, but it can be questioned whether this yields the most meaningful results given the exponential nature of epidemic processes and the several orders of magnitude that observed values can span over space and time. In this paper, we argue that log transforming counts before applying scores such as the CRPS or WIS can effectively mitigate these difficulties and yield epidemiologically meaningful and easily interpretable results. We motivate the procedure threefold using the CRPS on log-transformed counts as an example: Firstly, it can be interpreted as a probabilistic version of a relative error. Secondly, it reflects how well models predicted the time-varying epidemic growth rate. And lastly, using arguments on variance-stabilizing transformations, it can be shown that under the assumption of a quadratic mean-variance relationship, the logarithmic transformation leads to expected CRPS values which are independent of the order of magnitude of the predicted quantity. Applying the log transformation to data and forecasts from the European COVID-19 Forecast Hub, we find that it changes model rankings regardless of stratification by forecast date, location or target types. Situations in which models missed the beginning of upward swings are more strongly emphasized while failing to predict a downturn following a peak is less severely penalized. We conclude that appropriate transformations, of which the natural logarithm is only one particularly attractive option, should be considered when assessing the performance of different models in the context of infectious disease incidence.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C7J3FZBC/Bosse et al. - 2023 - Transformation of forecasts for evaluating predict.pdf}
}

@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS computational biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  pmcid = {PMC7880475},
  pmid = {33577550},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  annotation = {note: DOI 10.1371/journal.pcbi.1008618},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@article{bracherShorttermForecastingCOVID192021,
  title = {Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave \textendash{} a Preregistered Study},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, Nikos I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieli{\'n}ski, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = jan,
  journal = {medRxiv},
  pages = {2020.12.24.20248826},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.12.24.20248826},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}We report insights from ten weeks of collaborative COVID-19 forecasting for Germany and Poland (12 October \textendash{} 19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BHPBLCD9/Bracher et al. - 2021 - Short-term forecasting of COVID-19 in Germany and .pdf;/Users/nikos/github-synced/zotero-nikos/storage/I3ULULUZ/2020.12.24.20248826v2.html}
}

@article{cramerEvaluationIndividualEnsemble2021,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID-19}} Mortality in the {{US}}},
  author = {Cramer, Estee and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Rivadeneira, Alvaro J. Castro and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and M{\"u}hlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and Snyder, Timothy L. and Wilson, Davison D. and McConnell, Steve and Walraven, Robert and Shi, Yunfeng and Ban, Xuegang and Hong, Qi-Jun and Kong, Stanley and Turtle, James A. and {Ben-Nun}, Michal and Riley, Pete and Riley, Steven and Koyluoglu, Ugur and DesRoches, David and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Ozcan, Gokce and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Ferres, Juan Lavista and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and y Piontti, Ana Pastore and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Penna, Nicolas D. and Celi, Leo A. and Sundar, Saketh and Cavany, Sean and Espa{\~n}a, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and {Perez-Saez}, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Kinsey, Matt and Obrecht, R. F. and Tallaksen, Katharine and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gaikedu, Emmanuela and Hay, Simon and Lim, Steve and Murray, Chris and Pigott, David and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodr{\'i}guez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewel, Joel and Meakin, Sophie R. and Munday, James D. and Sherratt, Katherine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Li, Michael L. and Bertsimas, Dimitris and Lami, Omar Skali and Soni, Saksham and Bouardi, Hamza Tazi and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Hu, Addison J. and Jahja, Maria and Narasimhan, Balasubramanian and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O'Dea, Eamon B. and Drake, John M. and Pagano, Robert and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael and Biggerstaff, Matthew and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {medRxiv},
  pages = {2021.02.03.21250974},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2021.02.03.21250974},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. In 2020, the COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized hundreds of thousands of specific predictions from more than 50 different academic, industry, and independent research groups. This manuscript systematically evaluates 23 models that regularly submitted forecasts of reported weekly incident COVID-19 mortality counts in the US at the state and national level. One of these models was a multi-model ensemble that combined all available forecasts each week. The performance of individual models showed high variability across time, geospatial units, and forecast horizons. Half of the models evaluated showed better accuracy than a na\"ive baseline model. In combining the forecasts from all teams, the ensemble showed the best overall probabilistic accuracy of any model. Forecast accuracy degraded as models made predictions farther into the future, with probabilistic accuracy at a 20-week horizon more than 5 times worse than when predicting at a 1-week horizon. This project underscores the role that collaboration and active coordination between governmental public health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/W82X9ZN5/Cramer et al. - 2021 - Evaluation of individual and ensemble probabilisti.pdf;/Users/nikos/github-synced/zotero-nikos/storage/7MC6LGTC/2021.02.03.21250974v1.html}
}

@article{farrowHumanJudgmentApproach2017,
  title = {A Human Judgment Approach to Epidemiological Forecasting},
  author = {Farrow, David C. and Brooks, Logan C. and Hyun, Sangwon and Tibshirani, Ryan J. and Burke, Donald S. and Rosenfeld, Roni},
  year = {2017},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {3},
  pages = {e1005248},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005248},
  abstract = {Infectious diseases impose considerable burden on society, despite significant advances in technology and medicine over the past century. Advanced warning can be helpful in mitigating and preparing for an impending or ongoing epidemic. Historically, such a capability has lagged for many reasons, including in particular the uncertainty in the current state of the system and in the understanding of the processes that drive epidemic trajectories. Presently we have access to data, models, and computational resources that enable the development of epidemiological forecasting systems. Indeed, several recent challenges hosted by the U.S. government have fostered an open and collaborative environment for the development of these technologies. The primary focus of these challenges has been to develop statistical and computational methods for epidemiological forecasting, but here we consider a serious alternative based on collective human judgment. We created the web-based ``Epicast'' forecasting system which collects and aggregates epidemic predictions made in real-time by human participants, and with these forecasts we ask two questions: how accurate is human judgment, and how do these forecasts compare to their more computational, data-driven alternatives? To address the former, we assess by a variety of metrics how accurately humans are able to predict influenza and chikungunya trajectories. As for the latter, we show that real-time, combined human predictions of the 2014\textendash 2015 and 2015\textendash 2016 U.S. flu seasons are often more accurate than the same predictions made by several statistical systems, especially for short-term targets. We conclude that there is valuable predictive power in collective human judgment, and we discuss the benefits and drawbacks of this approach.},
  langid = {english},
  keywords = {Chikungunya infection,Epidemiological methods and statistics,Epidemiological statistics,Epidemiology,Forecasting,Infectious diseases,Influenza,Statistical methods},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N6MZGQ7M/Farrow et al. - 2017 - A human judgment approach to epidemiological forec.pdf;/Users/nikos/github-synced/zotero-nikos/storage/VSY8MVUC/article.html}
}

@article{fraserEstimatingIndividualHousehold2007,
  title = {Estimating {{Individual}} and {{Household Reproduction Numbers}} in an {{Emerging Epidemic}}},
  author = {Fraser, Christophe},
  year = {2007},
  month = aug,
  journal = {PLOS ONE},
  volume = {2},
  number = {8},
  pages = {e758},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0000758},
  abstract = {Reproduction numbers, defined as averages of the number of people infected by a typical case, play a central role in tracking infectious disease outbreaks. The aim of this paper is to develop methods for estimating reproduction numbers which are simple enough that they could be applied with limited data or in real time during an outbreak. I present a new estimator for the individual reproduction number, which describes the state of the epidemic at a point in time rather than tracking individuals over time, and discuss some potential benefits. Then, to capture more of the detail that micro-simulations have shown is important in outbreak dynamics, I analyse a model of transmission within and between households, and develop a method to estimate the household reproduction number, defined as the number of households infected by each infected household. This method is validated by numerical simulations of the spread of influenza and measles using historical data, and estimates are obtained for would-be emerging epidemics of these viruses. I argue that the household reproduction number is useful in assessing the impact of measures that target the household for isolation, quarantine, vaccination or prophylactic treatment, and measures such as social distancing and school or workplace closures which limit between-household transmission, all of which play a key role in current thinking on future infectious disease mitigation.},
  langid = {english},
  keywords = {Epidemiological methods and statistics,Epidemiology,Infectious disease epidemiology,Influenza,Measles,Measles virus,Pandemics,Respiratory infections},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MKA528M7/Fraser - 2007 - Estimating Individual and Household Reproduction N.pdf;/Users/nikos/github-synced/zotero-nikos/storage/A795DEET/article.html}
}

@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}

@article{linDiseaseSeverityClinical2021,
  title = {The {{Disease Severity}} and {{Clinical Outcomes}} of the {{SARS-CoV-2 Variants}} of {{Concern}}},
  author = {Lin, Lixin and Liu, Ying and Tang, Xiujuan and He, Daihai},
  year = {2021},
  journal = {Frontiers in Public Health},
  volume = {9},
  issn = {2296-2565},
  abstract = {With the continuation of the pandemic, many severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants have appeared around the world. Owing to a possible risk of increasing the transmissibility of the virus, severity of the infected individuals, and the ability to escape the antibody produced by the vaccines, the four SARS-CoV-2 variants of Alpha (B.1.1.7), Beta (B.1.351), Gamma (P.1), and Delta (B.1.617.2) have attracted the most widespread attention. At present, there is a unified conclusion that these four variants have increased the transmissibility of SARS-CoV-2, but the severity of the disease caused by them has not yet been determined. Studies from June 1, 2020 to October 15, 2021 were considered, and a meta-analysis was carried out to process the data. Alpha, Beta, Gamma, and Delta variants are all more serious than the wild-type virus in terms of hospitalization, ICU admission, and mortality, and the Beta and Delta variants have a higher risk than the Alpha and Gamma variants. Notably, the random effects of Beta variant to the wild-type virus with respect to hospitalization rate, severe illness rate, and mortality rate are 2.16 (95\% CI: 1.19\textendash 3.14), 2.23 (95\% CI: 1.31\textendash 3.15), and 1.50 (95\% CI: 1.26\textendash 1.74), respectively, and the random effects of Delta variant to the wild-type virus are 2.08 (95\% CI: 1.77\textendash 2.39), 3.35 (95\% CI: 2.5\textendash 4.2), and 2.33 (95\% CI: 1.45\textendash 3.21), respectively. Although, the emergence of vaccines may reduce the threat posed by SARS-CoV-2 variants, these are still very important, especially the Beta and Delta variants.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3WE3QDYT/Lin et al. - 2021 - The Disease Severity and Clinical Outcomes of the .pdf}
}

@article{mcandrewChimericForecastingCombining2022,
  title = {Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment},
  shorttitle = {Chimeric Forecasting},
  author = {McAndrew, Thomas and Codi, Allison and Cambeiro, Juan and Besiroglu, Tamay and Braun, David and Chen, Eva and De C{\`e}saris, Luis Enrique Urtubey and Luk, Damon},
  year = {2022},
  month = nov,
  journal = {BMC infectious diseases},
  volume = {22},
  number = {1},
  pages = {833},
  issn = {1471-2334},
  doi = {10.1186/s12879-022-07794-5},
  abstract = {Forecasts of the trajectory of an infectious agent can help guide public health decision making. A traditional approach to forecasting fits a computational model to structured data and generates a predictive distribution. However, human judgment has access to the same data as computational models plus experience, intuition, and subjective data. We propose a chimeric ensemble-a combination of computational and human judgment forecasts-as a novel approach to predicting the trajectory of an infectious agent. Each month from January, 2021 to June, 2021 we asked two generalist crowds, using the same criteria as the COVID-19 Forecast Hub, to submit a predictive distribution over incident cases and deaths at the US national level either two or three weeks into the future and combined these human judgment forecasts with forecasts from computational models submitted to the COVID-19 Forecasthub into a chimeric ensemble. We find a chimeric ensemble compared to an ensemble including only computational models improves predictions of incident cases and shows similar performance for predictions of incident deaths. A chimeric ensemble is a flexible, supportive public health tool and shows promising results for predictions of the spread of an infectious agent.},
  copyright = {cc by},
  langid = {english},
  pmcid = {PMC9648897},
  pmid = {36357829},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MECUBIMY/12879_2022_7794_MOESM1_ESM.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ULVZRCFS/McAndrew et al. - 2022 - Chimeric forecasting combining probabilistic pred.pdf}
}

@article{mcandrewExpertJudgmentModel2022,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID-19}} Pandemic in the {{United States}}},
  author = {McAndrew, Thomas and Reich, Nicholas G.},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010485},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010485},
  abstract = {From February to May 2020, experts in the modeling of infectious disease provided quantitative predictions and estimates of trends in the emerging COVID-19 pandemic in a series of 13 surveys. Data on existing transmission patterns were sparse when the pandemic began, but experts synthesized information available to them to provide quantitative, judgment-based assessments of the current and future state of the pandemic. We aggregated expert predictions into a single ``linear pool'' by taking an equally weighted average of their probabilistic statements. At a time when few computational models made public estimates or predictions about the pandemic, expert judgment provided (a) falsifiable predictions of short- and long-term pandemic outcomes related to reported COVID-19 cases, hospitalizations, and deaths, (b) estimates of latent viral transmission, and (c) counterfactual assessments of pandemic trajectories under different scenarios. The linear pool approach of aggregating expert predictions provided more consistently accurate predictions than any individual expert, although the predictive accuracy of a linear pool rarely provided the most accurate prediction. This work highlights the importance that an expert linear pool could play in flexibly assessing a wide array of risks early in future emerging outbreaks, especially in settings where available data cannot yet support data-driven computational modeling.},
  langid = {english},
  keywords = {Body weight,COVID 19,Forecasting,Pandemics,Probability distribution,Public and occupational health,SARS CoV 2,Surveys},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MX23NF3S/McAndrew and Reich - 2022 - An expert judgment model to predict early stages o.pdf}
}

@article{recchiaHowWellDid2021,
  title = {How Well Did Experts and Laypeople Forecast the Size of the {{COVID-19}} Pandemic?},
  author = {Recchia, Gabriel and Freeman, Alexandra L. J. and Spiegelhalter, David},
  year = {2021},
  month = may,
  journal = {PLOS ONE},
  volume = {16},
  number = {5},
  pages = {e0250935},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0250935},
  abstract = {Throughout the COVID-19 pandemic, social and traditional media have disseminated predictions from experts and nonexperts about its expected magnitude. How accurate were the predictions of `experts'\textemdash individuals holding occupations or roles in subject-relevant fields, such as epidemiologists and statisticians\textemdash compared with those of the public? We conducted a survey in April 2020 of 140 UK experts and 2,086 UK laypersons; all were asked to make four quantitative predictions about the impact of COVID-19 by 31 Dec 2020. In addition to soliciting point estimates, we asked participants for lower and higher bounds of a range that they felt had a 75\% chance of containing the true answer. Experts exhibited greater accuracy and calibration than laypersons, even when restricting the comparison to a subset of laypersons who scored in the top quartile on a numeracy test. Even so, experts substantially underestimated the ultimate extent of the pandemic, and the mean number of predictions for which the expert intervals contained the actual outcome was only 1.8 (out of 4), suggesting that experts should consider broadening the range of scenarios they consider plausible. Predictions of the public were even more inaccurate and poorly calibrated, suggesting that an important role remains for expert predictions as long as experts acknowledge their uncertainty.},
  langid = {english},
  keywords = {COVID 19,Epidemiological statistics,Forecasting,Numeracy,Pandemics,Probability distribution,Statistical distributions,Virus testing},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X2FMBK56/Recchia et al. - 2021 - How well did experts and laypeople forecast the si.pdf;/Users/nikos/github-synced/zotero-nikos/storage/TT7K727A/article.html}
}

@misc{sherrattExploringSurveillanceData2021,
  title = {Exploring Surveillance Data Biases When Estimating the Reproduction Number: With Insights into Subpopulation Transmission of {{Covid-19}} in {{England}}},
  shorttitle = {Exploring Surveillance Data Biases When Estimating the Reproduction Number},
  author = {Sherratt, Katharine and Abbott, Sam and Meakin, Sophie R. and Hellewell, Joel and Munday, James D. and Bosse, Nikos and working Group, CMMID Covid-19 and Jit, Mark and Funk, Sebastian},
  year = {2021},
  month = mar,
  pages = {2020.10.18.20214585},
  institution = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.10.18.20214585},
  abstract = {The time-varying reproduction number (Rt: the average number secondary infections caused by each infected person) may be used to assess changes in transmission potential during an epidemic. While new infections are not usually observed directly, they can be estimated from data. However, data may be delayed and potentially biased. We investigated the sensitivity of Rt estimates to different data sources representing Covid-19 in England, and we explored how this sensitivity could track epidemic dynamics in population sub-groups. We sourced public data on test-positive cases, hospital admissions, and deaths with confirmed Covid-19 in seven regions of England over March through August 2020. We estimated Rt using a model that mapped unobserved infections to each data source. We then compared differences in Rt with the demographic and social context of surveillance data over time. Our estimates of transmission potential varied for each data source, with the relative inconsistency of estimates varying across regions and over time. Rt estimates based on hospital admissions and deaths were more spatio-temporally synchronous than when compared to estimates from all test-positives. We found these differences may be linked to biased representations of subpopulations in each data source. These included spatially clustered testing, and where outbreaks in hospitals, care homes, and young age groups reflected the link between age and severity of disease. We highlight that policy makers could better target interventions by considering the source populations of Rt estimates. Further work should clarify the best way to combine and interpret Rt estimates from different data sources based on the desired use.},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/Y8CPF3U3/Sherratt et al. - 2021 - Exploring surveillance data biases when estimating.pdf;/Users/nikos/github-synced/zotero-nikos/storage/HGFVXLM7/2020.10.18.html}
}

@misc{sherrattPredictivePerformanceMultimodel2022a,
  title = {Predictive Performance of Multi-Model Ensemble Forecasts of {{COVID-19}} across  {{European}} Nation},
  author = {Sherratt, K. and Gruson, H. and Grah, R. and Johnson, H. and Niehus, R. and Prasse, B. and Sandman, F. and Deuschel, J. and Wolffram, D. and Abbott, S. and Ullrich, A. and Gibson, G. and Ray, EL. and Reich, NG. and Sheldon, D. and Wang, Y. and Wattanachit, N. and Wang, L. and Trnka, J. and Obozinski, G. and Sun, T. and Thanou, D. and Pottier, L. and Krymova, E. and Barbarossa, MV. and Leith{\"a}user, N. and Mohring, J. and Schneider, J. and Wlazlo, J. and Fuhrmann, J. and Lange, B. and Rodiah, I. and Baccam, P. and Gurung, H. and Stage, S. and Suchoski, B. and Budzinski, J. and Walraven, R. and Villanueva, I. and Tucek, V. and {\v S}m{\'i}d, M. and Zaj{\'i}cek, M. and P{\'e}rez, {\'A}lvarez C. and Reina, B. and Bosse, NI. and Meakin, S. and Di Loro, Alaimo and Maruotti, A. and Eclerov{\'a}, V. and Kraus, A. and Kraus, D. and Pribylova, L. and Dimitris, B. and Li, ML. and Saksham, S. and Dehning, J. and Mohr, S. and Priesemann, V. and Redlarski, G. and Bejar, B. and Ardenghi, G. and Parolini, N. and Ziarelli, G. and Bock, W. and Heyder, S. and Hotz, T. and E., Singh D. and {Guzman-Merino}, M. and Aznarte, JL. and Mori{\~n}a, D. and Alonso, S. and {\'A}lvarez, E. and L{\'o}pez, D. and Prats, C. and Burgard, JP. and Rodloff, A. and Zimmermann, T. and Kuhlmann, A. and Zibert, J. and Pennoni, F. and Divino, F. and Catal{\`a}, M. and Lovison, G. and Giudici, P. and Tarantino, B. and Bartolucci, F. and Jona, Lasinio G. and Mingione, M. and Farcomeni, A. and Srivastava, A. and {Montero-Manso}, P. and Adiga, A. and Hurt, B. and Lewis, B. and Marathe, M. and Porebski, P. and Venkatramanan, S. and Bartczuk, R. and Dreger, F. and Gambin, A. and Gogolewski, K. and {Gruziel-Slomka}, M. and Krupa, B. and Moszynski, A. and Niedzielewski, K. and Nowosielski, J. and Radwan, M. and Rakowski, F. and Semeniuk, M. and Szczurek, E. and Zielinski, J. and Kisielewski, J. and Pabjan, B. and Holger, K. and Kheifetz, Y. and Scholz, M. and Bodych, M. and Filinski, M. and Idzikowski, R. and Krueger, T. and Ozanski, T. and Bracher, J. and Funk, S.},
  year = {2022},
  doi = {10.1101/2022.06.16.22276024},
  abstract = {Background  Short-term forecasts of infectious disease burden can contribute to  situational awareness and aid capacity planning. Based on best practice in  other fields and recent insights in infectious disease epidemiology, one can  maximise the predictive performance of such forecasts if multiple models are  combined into an ensemble. Here we report on the performance of ensembles in  predicting COVID-19 cases and deaths across Europe between 08 March 2021 and  07 March 2022. Methods  We used open-source tools to develop a public European COVID-19 Forecast  Hub. We invited groups globally to contribute weekly forecasts for COVID-19  cases and deaths reported from a standardised source over the next one to  four weeks. Teams submitted forecasts from March 2021 using standardised  quantiles of the predictive distribution. Each week we created an ensemble  forecast, where each predictive quantile was calculated as the  equally-weighted average (initially the mean and then from 26th July the  median) of all individual models' predictive quantiles. We measured the  performance of each model using the relative Weighted Interval Score (WIS),  comparing models' forecast accuracy relative to all other models. We  retrospectively explored alternative methods for ensemble forecasts,  including weighted averages based on models' past predictive  performance. Results  Over 52 weeks we collected and combined up to 28 forecast models for 32  countries. We found a weekly ensemble had a consistently strong performance  across countries over time. Across all horizons and locations, the ensemble  performed better on relative WIS than 84\% of participating models' forecasts  of incident cases (with a total N=862), and 92\% of participating models'  forecasts of deaths (N=746). Across a one to four week time horizon,  ensemble performance declined with longer forecast periods when forecasting  cases, but remained stable over four weeks for incident death forecasts. In  every forecast across 32 countries, the ensemble outperformed most  contributing models when forecasting either cases or deaths, frequently  outperforming all of its individual component models. Among several choices  of ensemble methods we found that the most influential and best choice was  to use a median average of models instead of using the mean, regardless of  methods of weighting component forecast models. Conclusions  Our results support the use of combining forecasts from individual  models into an ensemble in order to improve predictive performance across  epidemiological targets and populations during infectious disease epidemics.  Our findings further suggest that median ensemble methods yield better  predictive performance more than ones based on means. Our findings also  highlight that forecast consumers should place more weight on incident death  forecasts than incident case forecasts at forecast horizons greater than two  weeks. Code and data availability  All data and code are publicly available on Github:  covid19-forecast-hub-europe/euro-hub-ensemble.},
  copyright = {cc by},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HK3RSINT/Sherratt et al. - 2022 - Predictive performance of multi-model ensemble for.pdf}
}

@misc{venkatramananUtilityHumanJudgment2022,
  title = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty: {{A}} Case Study during the {{COVID-19 Omicron BA}}.1 Wave in the {{USA}}},
  shorttitle = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty},
  author = {Venkatramanan, Srinivasan and Cambeiro, Juan and Liptay, Tom and Lewis, Bryan and Orr, Mark and Dempsey, Gaia and Telionis, Alex and Crow, Justin and Barrett, Chris and Marathe, Madhav},
  year = {2022},
  month = oct,
  pages = {2022.10.12.22280997},
  publisher = {{medRxiv}},
  doi = {10.1101/2022.10.12.22280997},
  abstract = {Responding to a rapidly evolving pandemic like COVID-19 is challenging, and involves anticipating novel variants, vaccine uptake, and behavioral adaptations. Human judgment systems can complement computational models by providing valuable real-time forecasts. We report findings from a study conducted on Metaculus, a community forecasting platform, in partnership with the Virginia Department of Health, involving six rounds of forecasting during the Omicron BA.1 wave in the United States from November 2021 to March 2022. We received 8355 probabilistic predictions from 129 unique users across 60 questions pertaining to cases, hospitalizations, vaccine uptake, and peak/trough activity. We observed that the case forecasts performed on par with national multi-model ensembles and the vaccine uptake forecasts were more robust and accurate compared to baseline models. We also identified qualitative shifts in Omicron BA.1 wave prognosis during the surge phase, demonstrating rapid adaptation of such systems. Finally, we found that community estimates of variant characteristics such as growth rate and timing of dominance were in line with the scientific consensus. The observed accuracy, timeliness, and scope of such systems demonstrates the value of incorporating them into pandemic policymaking workflows.},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HG7AGF5X/Venkatramanan et al. - 2022 - Utility of human judgment ensembles during times o.pdf}
}
